{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11074658,"sourceType":"datasetVersion","datasetId":6901910}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torchvision import datasets, transforms\nimport pathlib\nfrom torch.amp import GradScaler, autocast\nimport time\nimport math\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Enhanced Data Augmentation\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Dataset\ntraining_dir = pathlib.Path('/kaggle/input/radar-signal-classification/training_set')\nfull_dataset = datasets.ImageFolder(root=str(training_dir), transform=transform)\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\nclass_names = full_dataset.classes\nnum_classes = len(class_names)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n\nprint(f\"Classes: {class_names}\")\nprint(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n\n# Optimized CNN Model\nclass RadarSignalCNN(nn.Module):\n    def __init__(self, num_classes=8):\n        super(RadarSignalCNN, self).__init__()\n        \n        # Feature extraction\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.2),\n            \n            # Block 2\n            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.3),\n            \n            # Block 3\n            nn.Conv2d(64, 120, kernel_size=3, padding=1, bias=False),  # Giáº£m tá»« 128 xuá»‘ng 120\n            nn.BatchNorm2d(120),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(120, 120, kernel_size=3, padding=1, bias=False),  # Giáº£m tá»« 128 xuá»‘ng 120\n            nn.BatchNorm2d(120),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(120, 120),  # Giáº£m tá»« 128 xuá»‘ng 120\n            nn.BatchNorm1d(120),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(120, num_classes)\n        )\n        \n        # Initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return F.log_softmax(x, dim=1)\n\n# Model initialization\nmodel = RadarSignalCNN(num_classes=num_classes).to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"ðŸ”¢ Total trainable parameters: {total_params}\")\n\n# Enhanced training configuration\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, \n                                        steps_per_epoch=len(train_loader), \n                                        epochs=50, \n                                        anneal_strategy='cos')\nscaler = GradScaler('cuda')\n\n# Training function without early stopping\ndef train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):\n    best_val_acc = 0.0\n    \n    for epoch in range(epochs):\n        start_time = time.time()\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            \n            with autocast(device_type='cuda'):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                with autocast(device_type='cuda'):\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100. * correct / total\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_model.pt\")\n        \n        epoch_time = time.time() - start_time\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        print(f\"Epoch {epoch+1:2d} | \"\n              f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n              f\"Train Acc: {train_acc:.2f}% | \"\n              f\"Val Loss: {val_loss/len(val_loader):.4f} | \"\n              f\"Val Acc: {val_acc:.2f}% | \"\n              f\"Time: {epoch_time:.2f}s | \"\n              f\"LR: {current_lr:.6f}\")\n    \n    return model\n\n# Train the model\ntorch.cuda.empty_cache()\nmodel = train(model, train_loader, val_loader, criterion, optimizer, scheduler)\n\n# Save the model\nexample_input = torch.randn(1, 3, 128, 128).to(device)\ntraced_model = torch.jit.trace(model, example_input)\ntraced_model.save(\"22119129_22119067.pt\")\nprint(\"âœ… Model saved as 22119129_22119067.pt\")\n\ntorch.cuda.empty_cache()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:17:54.053956Z","iopub.execute_input":"2025-04-28T06:17:54.054705Z","iopub.status.idle":"2025-04-28T06:24:40.605820Z","shell.execute_reply.started":"2025-04-28T06:17:54.054673Z","shell.execute_reply":"2025-04-28T06:24:40.604873Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nClasses: ['B-FM', 'Barker', 'CPFSK', 'DSB-AM', 'GFSK', 'LFM', 'Rect', 'SSB-AM']\nTraining samples: 5120, Validation samples: 1280\nðŸ”¢ Total trainable parameters: 280688\nEpoch  1 | Train Loss: 1.9511 | Train Acc: 27.75% | Val Loss: 1.8272 | Val Acc: 26.88% | Time: 8.09s | LR: 0.000051\nEpoch  2 | Train Loss: 1.7475 | Train Acc: 33.95% | Val Loss: 1.7462 | Val Acc: 30.94% | Time: 8.06s | LR: 0.000082\nEpoch  3 | Train Loss: 1.6504 | Train Acc: 38.07% | Val Loss: 1.6962 | Val Acc: 31.41% | Time: 7.94s | LR: 0.000132\nEpoch  4 | Train Loss: 1.5551 | Train Acc: 44.08% | Val Loss: 1.6466 | Val Acc: 36.25% | Time: 8.38s | LR: 0.000199\nEpoch  5 | Train Loss: 1.4629 | Train Acc: 49.14% | Val Loss: 1.6357 | Val Acc: 43.28% | Time: 7.97s | LR: 0.000280\nEpoch  6 | Train Loss: 1.3412 | Train Acc: 54.69% | Val Loss: 1.4564 | Val Acc: 47.89% | Time: 7.93s | LR: 0.000372\nEpoch  7 | Train Loss: 1.1982 | Train Acc: 63.11% | Val Loss: 1.3804 | Val Acc: 49.14% | Time: 7.91s | LR: 0.000470\nEpoch  8 | Train Loss: 1.0731 | Train Acc: 69.26% | Val Loss: 1.1613 | Val Acc: 63.52% | Time: 8.46s | LR: 0.000571\nEpoch  9 | Train Loss: 1.0119 | Train Acc: 71.88% | Val Loss: 1.0495 | Val Acc: 68.67% | Time: 8.09s | LR: 0.000669\nEpoch 10 | Train Loss: 0.9682 | Train Acc: 74.59% | Val Loss: 1.1006 | Val Acc: 67.66% | Time: 7.97s | LR: 0.000761\nEpoch 11 | Train Loss: 0.9262 | Train Acc: 77.19% | Val Loss: 1.0114 | Val Acc: 73.83% | Time: 7.91s | LR: 0.000842\nEpoch 12 | Train Loss: 0.8960 | Train Acc: 78.61% | Val Loss: 0.9261 | Val Acc: 76.72% | Time: 8.35s | LR: 0.000909\nEpoch 13 | Train Loss: 0.8740 | Train Acc: 79.63% | Val Loss: 0.9185 | Val Acc: 75.55% | Time: 7.96s | LR: 0.000959\nEpoch 14 | Train Loss: 0.8680 | Train Acc: 80.45% | Val Loss: 0.8958 | Val Acc: 80.47% | Time: 8.18s | LR: 0.000990\nEpoch 15 | Train Loss: 0.8438 | Train Acc: 81.05% | Val Loss: 0.8686 | Val Acc: 80.62% | Time: 8.06s | LR: 0.001000\nEpoch 16 | Train Loss: 0.8371 | Train Acc: 82.03% | Val Loss: 0.9615 | Val Acc: 74.69% | Time: 8.66s | LR: 0.000998\nEpoch 17 | Train Loss: 0.8236 | Train Acc: 82.32% | Val Loss: 0.8159 | Val Acc: 83.91% | Time: 8.12s | LR: 0.000992\nEpoch 18 | Train Loss: 0.8282 | Train Acc: 81.88% | Val Loss: 0.8384 | Val Acc: 82.73% | Time: 7.98s | LR: 0.000982\nEpoch 19 | Train Loss: 0.8043 | Train Acc: 83.30% | Val Loss: 0.8005 | Val Acc: 85.23% | Time: 7.94s | LR: 0.000968\nEpoch 20 | Train Loss: 0.7969 | Train Acc: 83.40% | Val Loss: 0.7572 | Val Acc: 86.88% | Time: 8.36s | LR: 0.000950\nEpoch 21 | Train Loss: 0.7965 | Train Acc: 83.24% | Val Loss: 0.7734 | Val Acc: 84.45% | Time: 7.90s | LR: 0.000929\nEpoch 22 | Train Loss: 0.7916 | Train Acc: 83.91% | Val Loss: 0.8529 | Val Acc: 81.56% | Time: 8.07s | LR: 0.000904\nEpoch 23 | Train Loss: 0.7799 | Train Acc: 84.41% | Val Loss: 0.7519 | Val Acc: 86.02% | Time: 8.05s | LR: 0.000876\nEpoch 24 | Train Loss: 0.7732 | Train Acc: 84.12% | Val Loss: 0.7440 | Val Acc: 85.55% | Time: 8.64s | LR: 0.000845\nEpoch 25 | Train Loss: 0.7705 | Train Acc: 84.94% | Val Loss: 0.7767 | Val Acc: 84.38% | Time: 8.06s | LR: 0.000811\nEpoch 26 | Train Loss: 0.7709 | Train Acc: 85.00% | Val Loss: 0.7343 | Val Acc: 85.70% | Time: 8.06s | LR: 0.000775\nEpoch 27 | Train Loss: 0.7668 | Train Acc: 84.69% | Val Loss: 0.7614 | Val Acc: 86.02% | Time: 7.97s | LR: 0.000736\nEpoch 28 | Train Loss: 0.7656 | Train Acc: 84.92% | Val Loss: 0.7593 | Val Acc: 86.95% | Time: 8.48s | LR: 0.000696\nEpoch 29 | Train Loss: 0.7674 | Train Acc: 85.29% | Val Loss: 0.8174 | Val Acc: 82.50% | Time: 8.11s | LR: 0.000654\nEpoch 30 | Train Loss: 0.7495 | Train Acc: 86.19% | Val Loss: 0.7722 | Val Acc: 83.05% | Time: 8.22s | LR: 0.000611\nEpoch 31 | Train Loss: 0.7535 | Train Acc: 85.25% | Val Loss: 0.7619 | Val Acc: 85.00% | Time: 8.15s | LR: 0.000567\nEpoch 32 | Train Loss: 0.7494 | Train Acc: 85.74% | Val Loss: 0.7279 | Val Acc: 88.44% | Time: 8.53s | LR: 0.000522\nEpoch 33 | Train Loss: 0.7465 | Train Acc: 86.04% | Val Loss: 0.7473 | Val Acc: 84.45% | Time: 7.90s | LR: 0.000477\nEpoch 34 | Train Loss: 0.7471 | Train Acc: 86.19% | Val Loss: 0.7490 | Val Acc: 85.94% | Time: 7.91s | LR: 0.000432\nEpoch 35 | Train Loss: 0.7394 | Train Acc: 86.17% | Val Loss: 0.7509 | Val Acc: 84.77% | Time: 7.97s | LR: 0.000388\nEpoch 36 | Train Loss: 0.7295 | Train Acc: 86.48% | Val Loss: 0.7352 | Val Acc: 85.62% | Time: 8.62s | LR: 0.000345\nEpoch 37 | Train Loss: 0.7329 | Train Acc: 87.03% | Val Loss: 0.7218 | Val Acc: 86.88% | Time: 8.21s | LR: 0.000303\nEpoch 38 | Train Loss: 0.7322 | Train Acc: 87.07% | Val Loss: 0.7195 | Val Acc: 86.95% | Time: 8.14s | LR: 0.000263\nEpoch 39 | Train Loss: 0.7222 | Train Acc: 87.34% | Val Loss: 0.6987 | Val Acc: 88.05% | Time: 8.09s | LR: 0.000224\nEpoch 40 | Train Loss: 0.7221 | Train Acc: 87.25% | Val Loss: 0.6990 | Val Acc: 88.12% | Time: 8.36s | LR: 0.000188\nEpoch 41 | Train Loss: 0.7258 | Train Acc: 86.82% | Val Loss: 0.6993 | Val Acc: 88.28% | Time: 7.91s | LR: 0.000154\nEpoch 42 | Train Loss: 0.7189 | Train Acc: 87.11% | Val Loss: 0.7101 | Val Acc: 86.56% | Time: 7.89s | LR: 0.000123\nEpoch 43 | Train Loss: 0.7255 | Train Acc: 87.29% | Val Loss: 0.7031 | Val Acc: 86.95% | Time: 7.98s | LR: 0.000095\nEpoch 44 | Train Loss: 0.7168 | Train Acc: 87.66% | Val Loss: 0.6960 | Val Acc: 87.81% | Time: 8.60s | LR: 0.000070\nEpoch 45 | Train Loss: 0.7127 | Train Acc: 87.48% | Val Loss: 0.7002 | Val Acc: 88.20% | Time: 7.99s | LR: 0.000049\nEpoch 46 | Train Loss: 0.7100 | Train Acc: 87.58% | Val Loss: 0.7021 | Val Acc: 88.28% | Time: 7.99s | LR: 0.000032\nEpoch 47 | Train Loss: 0.7089 | Train Acc: 88.18% | Val Loss: 0.6969 | Val Acc: 87.97% | Time: 7.98s | LR: 0.000018\nEpoch 48 | Train Loss: 0.7139 | Train Acc: 88.24% | Val Loss: 0.6970 | Val Acc: 89.38% | Time: 8.37s | LR: 0.000008\nEpoch 49 | Train Loss: 0.7077 | Train Acc: 88.40% | Val Loss: 0.6964 | Val Acc: 88.59% | Time: 7.94s | LR: 0.000002\nEpoch 50 | Train Loss: 0.7069 | Train Acc: 87.91% | Val Loss: 0.6952 | Val Acc: 88.28% | Time: 7.84s | LR: 0.000000\nâœ… Model saved as 22119129_22119067.pt\n","output_type":"stream"}],"execution_count":31}]}